{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python_live_session_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Ijg5wUCTQYG"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
        "</p>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "## **Applied Machine Learning - Ensemble Modeling Live Training**\n",
        "\n",
        "Welcome to this hands-on training where you will immerse yourself in applied machine learning in Python where we'll explore model stacking. Using `sklearn.ensemble`, we'll learn how to create layers that are stacking-ready.\n",
        "\n",
        "The foundations of model stacking:\n",
        "\n",
        "* Create various types of baseline models, including linear and logistic regression using Scikit-Learn, for comparison to ensemble methods.\n",
        "* Build layers, then stack them up.\n",
        "* Calculate performance metrics.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## **1st Dataset**\n",
        "\n",
        "\n",
        "The first dataset we'll use is a CSV file named `pima-indians-diabetes.csv`, which contains data on females of Pima Indian heritage that are at least 21 years old. It contains the following columns:\n",
        "\n",
        "- `n_preg`: Number of pregnancies\n",
        "- `pl_glucose`: Plasma glucose concentration 2 hours after an oral glucose tolerance test\n",
        "- `dia_bp`: Diastolic blood pressure (mm Hg)\n",
        "- `tri_thick`: Triceps skin fold thickness (mm)\n",
        "- `serum_ins`: 2-Hour serum insulin (mu U/ml)\n",
        "- `bmi`: Body mass index (weight in kg/(height in m)^2)\n",
        "- `diab_ped`: Diabetes pedigree function\n",
        "- `age`: Age (years)\n",
        "- `class`: Class variable (0 or 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMQfyC7GUNhT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d5e5507e-41a6-4a8c-bf28-ad4c1282efd3"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l8t_EwRNZPLB",
        "colab": {}
      },
      "source": [
        "# Read in the dataset as Pandas DataFrame\n",
        "diabetes = pd.read_csv('https://github.com/datacamp/Applied-Machine-Learning-Ensemble-Modeling-live-training/blob/master/data/pima-indians-diabetes.csv?raw=true')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRJPuinPZpGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "790306ed-1e88-474d-9388-a7c80ffa6753"
      },
      "source": [
        "# Look at data using the info() function\n",
        "diabetes.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   n_preg      768 non-null    int64  \n",
            " 1   pl_glucose  768 non-null    int64  \n",
            " 2   dia_bp      768 non-null    int64  \n",
            " 3   tri_thick   768 non-null    int64  \n",
            " 4   serum_ins   768 non-null    int64  \n",
            " 5   bmi         768 non-null    float64\n",
            " 6   diab_ped    768 non-null    float64\n",
            " 7   age         768 non-null    int64  \n",
            " 8   class       768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OVOkU80oKP",
        "colab_type": "text"
      },
      "source": [
        "## **Observations:** \n",
        "- The `info()` function is critical to beginning to understand your data.  Here, there are no missing values.  However, that is not typical.\n",
        "- There is a mixture of integers and floats with the first 5 columns being `int64`, the next 2 `float64` and the last 2 'int64`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UtlpG_Zo50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "545283ea-b9ca-45bb-bb39-0f0fc4eb6aa3"
      },
      "source": [
        "# Read in the dataset as Pandas DataFrame\n",
        "# Look at data using the info() function\n",
        "# Look at data using the describe() function\n",
        "diabetes.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_preg</th>\n",
              "      <th>pl_glucose</th>\n",
              "      <th>dia_bp</th>\n",
              "      <th>tri_thick</th>\n",
              "      <th>serum_ins</th>\n",
              "      <th>bmi</th>\n",
              "      <th>diab_ped</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           n_preg  pl_glucose      dia_bp  ...    diab_ped         age       class\n",
              "count  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\n",
              "mean     3.845052  120.894531   69.105469  ...    0.471876   33.240885    0.348958\n",
              "std      3.369578   31.972618   19.355807  ...    0.331329   11.760232    0.476951\n",
              "min      0.000000    0.000000    0.000000  ...    0.078000   21.000000    0.000000\n",
              "25%      1.000000   99.000000   62.000000  ...    0.243750   24.000000    0.000000\n",
              "50%      3.000000  117.000000   72.000000  ...    0.372500   29.000000    0.000000\n",
              "75%      6.000000  140.250000   80.000000  ...    0.626250   41.000000    1.000000\n",
              "max     17.000000  199.000000  122.000000  ...    2.420000   81.000000    1.000000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCK9W_gk1HG8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **Observations:** \n",
        "- The `.describe()` function gives the summary statistics of the data.  Notice that the min of the 1st six columns is zero.  Even though there are no missing values, this is indicative of the measurements for those features having not been captured.\n",
        "- Although we previously saw there is a mixture of integer and float data types (as seen with `.info()`), the printout makes it appear as if all values are float.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE5F_JUQ2X-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6d629fb7-b53d-4d87-e306-455295062d21"
      },
      "source": [
        "# Print the first 5 rows of the data using the head() function\n",
        "diabetes.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_preg</th>\n",
              "      <th>pl_glucose</th>\n",
              "      <th>dia_bp</th>\n",
              "      <th>tri_thick</th>\n",
              "      <th>serum_ins</th>\n",
              "      <th>bmi</th>\n",
              "      <th>diab_ped</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   n_preg  pl_glucose  dia_bp  tri_thick  serum_ins   bmi  diab_ped  age  class\n",
              "0       6         148      72         35          0  33.6     0.627   50      1\n",
              "1       1          85      66         29          0  26.6     0.351   31      0\n",
              "2       8         183      64          0          0  23.3     0.672   32      1\n",
              "3       1          89      66         23         94  28.1     0.167   21      0\n",
              "4       0         137      40         35        168  43.1     2.288   33      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2VCIx0K2bT1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **Observation:**\n",
        "- Printing out the first 5 rows, we see that the data types of the columns are indeed as stated previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajAzhMDc2b1D",
        "colab_type": "text"
      },
      "source": [
        "## Let's check the number in each class:\n",
        "\n",
        "This avoids getting surprised by great results that are actually a side effect of class imbalance.  This happens when the majority class far outweighs the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKeXN3441-9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "affcc451-b87b-427c-faf7-dc3ca66c984e"
      },
      "source": [
        "# Summarize class distribution\n",
        "target = diabetes['class']\n",
        "counter = Counter(target)\n",
        "print(counter)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 500, 1: 268})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOpbGyQw55v3",
        "colab_type": "text"
      },
      "source": [
        "## **Observation:** For every two negative cases there is one positive case, not enough of a difference to be considered class imbalance.  \n",
        "- Class imbalance tends to exist when the majority class is > 90% although there is no hard and fast rule about this threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5XaYl9ZZ8B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert Pandas DataFrame to numpy array - Return only the values of the DataFrame with DataFrame.to_numpy()\n",
        "diabetes = diabetes.to_numpy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlGa9IBc7Gsr",
        "colab_type": "text"
      },
      "source": [
        "### Always verify that your X matrix and target array have the same number of rows to avoid errors during model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FEvD6Ab6InP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ad34928-a4eb-449d-df3b-223af36705a8"
      },
      "source": [
        "# Create X matrix and y (target) array using slicing [row_start:row_end, col_start:target_col],[row_start:row_end, target_col]\n",
        "X, y = diabetes[:, :-1], diabetes[:, -1]\n",
        "\n",
        "# Print X matrix and y (target) array dimensions using .shape \n",
        "print('Shape: %s, %s' % (X.shape, y.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (768, 8), (768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtDjfls8xbK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ALWAYS set a seed for reproducibility!  Do this using np.random.seed()\n",
        "np.random.seed(42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoI7t4U-Z8LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert X matrix data types to 'float32' for consistency using .astype()\n",
        "X = X.astype('float32')\n",
        "\n",
        "# Convert y (target) array to 'str' using .astype()\n",
        "y = y.astype('str')\n",
        "\n",
        "# Encode class labels in y array using dot notation with LabelEncoder().fit_transform()\n",
        "# Hint: y goes in the fit_transform function call\n",
        "y = LabelEncoder().fit_transform(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXWv2xp9v1q",
        "colab_type": "text"
      },
      "source": [
        "### Don't let the `.astype('str')` throw you!  This is simply taking the class labels and label encoding them – regardless of their original format.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHHu8uz7_yVa",
        "colab_type": "text"
      },
      "source": [
        "## **Creating a Naive Classifier**\n",
        "Here we'll use the `DummyClassifier` from `sklearn`.  This creates a so-called 'naive' classifer and is simply a model that predicts a single class for all of the rows, regardless of their original class.  \n",
        "\n",
        "1. `DummyClassifier()` arguments:\n",
        " - `strategy`: Strategy to use to generate predictions.\n",
        "\n",
        "2. `RepeatedStratifiedKFold()` arguments:\n",
        " - `n_splits`: Number of folds.\n",
        " - `n_repeats`: Number of times cross-validator needs to be repeated.\n",
        " - `random_state`: Controls the generation of the random states for each repetition. Pass an int for reproducible output across multiple function calls.  (This is an equivalent argument to np.random.seed above, but will be specific to this naive model.)\n",
        "\n",
        "3. `cross_val_score()` arguments:\n",
        " - The model to use.\n",
        " - The data to fit. (X)\n",
        " - The target variable to try to predict. (y)\n",
        " - `scoring`: A single string scorer callable object/function such as 'accuracy' or 'roc_auc'.  See https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter for more options.\n",
        " - `cv`: Cross-validation splitting strategy (default is 5)\n",
        " - `n_jobs`: Number of CPU cores used when parallelizing.  Set to -1 helps to avoid non-convergence errors.\n",
        " - `error_score`: Value to assign to the score if an error occurs in estimator fitting. If set to ‘raise’, the error is raised. If a numeric value is given, FitFailedWarning is raised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL4huFGPZ8RA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "824acd03-1451-4c25-f15d-11724f143b1c"
      },
      "source": [
        "# Evaluate naive\n",
        "\n",
        "# Instantiate a DummyClassifier with 'most_frequent' strategy\n",
        "naive = DummyClassifier(strategy='most_frequent')\n",
        "\n",
        "# Create RepeatedStratifiedKFold cross-validator with 10 folds, 3 repeats and a seed of 1.\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'accuracy' scoring, cross validator, n_jobs=-1, and error_score set to 'raise'\n",
        "n_scores = cross_val_score(naive, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# Print mean and standard deviation of n_scores: \n",
        "print('Naive score: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive score: 0.651 (0.003)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tEgwsOfsoB6",
        "colab_type": "text"
      },
      "source": [
        "## **Observation** \n",
        "- We want to do better than 65% accuracy to consider any other models as an improvement to a totally naive model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8QZOyg8s1eQ",
        "colab_type": "text"
      },
      "source": [
        "## **Creating a Baseline Classifier**\n",
        "Now we'll create a baseline classifier, one that seeks to correctly predict the class that each observation belongs to.  Since the target variable is binary, we'll instantiate a `LogisticRegression` model.\n",
        "\n",
        "1. `LogisticRegression()` arguments:\n",
        " - `solver`: Algorithm to use in the optimization problem. The `lbfgs` solver finds a (local) minimum of an objective function by using the gradient and values of the objective function.\n",
        " - `penalty`:  Used to specify the norm used in the penalization. The `l2` penalty is equivalent to Ridge Regression, shrinking the coefficients of the less important features toward (but not to) zero.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QczFUGSfbQvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95013195-3b2d-4e09-e1fe-7b0406a8a7b8"
      },
      "source": [
        "# Evaluate baseline model\n",
        "\n",
        "# Instantiate a LogisticRegression with 'lbfgs' solver and 'l2' penalty\n",
        "model = LogisticRegression(solver='lbfgs',penalty='l2')\n",
        "\n",
        "# Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'accuracy' scoring, cross validator 'cv', and error_score set to 'raise'\n",
        "m_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# Print mean and standard deviation of m_scores: \n",
        "print('Baseline score: %.3f (%.3f)' % (mean(m_scores), std(m_scores)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline score: 0.773 (0.040)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRUBiqqmtNA6",
        "colab_type": "text"
      },
      "source": [
        "## **Observation**\n",
        "- We want to do better than 77% with a Stacking Classifier to consider it an improvement over this baseline logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMYfcKeDY85K"
      },
      "source": [
        "## **Getting started with Stacking Classifier**\n",
        "\n",
        "- We're going to compare several additional baseline classifiers to see if they perform better than Logistic Regression we just trained previously.\n",
        "- We'll start by importing additional packages that we'll need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCHmx7k5NeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import several other classifiers for ensemble\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import StackingClassifier"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teQMB0aWxhcN",
        "colab_type": "text"
      },
      "source": [
        "## Create custom functions\n",
        "1. get_stacking() - This function will create the layers of our `StackingClassifier()`.\n",
        "2. get_models() - This function will create a dictionary of models to be evaluated.\n",
        "3. evaluate_model() - This function will evaluate each of the models to be compared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqtHxQFPvMqu",
        "colab_type": "text"
      },
      "source": [
        "## Custom function # 1: get_stacking()\n",
        "1. `StackingClassifier()` arguments:\n",
        " - `estimators`: List of baseline classifiers\n",
        " - `final_estimator`: Defined meta classifier \n",
        " - `cv`: Number of cross validations to perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFhBv6jR6FOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_stacking():\n",
        "def get_stacking():\n",
        "\n",
        "\t# Create an empty list for the base models called layer1\n",
        "  layer1 = list()\n",
        "\n",
        "  # Append tuple with classifier name and instantiations (no arguments) for LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, SVC, and GaussianNB base models\n",
        "  # Hint: layer1.append(('ModelName', Classifier()))\n",
        "  layer1.append(('LR', LogisticRegression()))\n",
        "  layer1.append(('KNN', KNeighborsClassifier()))\n",
        "  layer1.append(('DT', DecisionTreeClassifier()))\n",
        "  layer1.append(('SVM', SVC()))\n",
        "  layer1.append(('Bayes', GaussianNB()))\n",
        "\n",
        "  # Instantiate Logistic Regression as meta learner model called layer2\n",
        "  layer2 = LogisticRegression()\n",
        "\n",
        "\t# Define StackingClassifier() called model passing layer1 model list and meta learner with 5 cross-validations\n",
        "  model = StackingClassifier(estimators=layer1, final_estimator=layer2, cv=5)\n",
        "\n",
        "  # return model\n",
        "  return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5szw9liyaxp",
        "colab_type": "text"
      },
      "source": [
        "## Custom function # 2: get_models()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hEJlDLB4kv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_models():\n",
        "def get_models():\n",
        "\n",
        "  # Create empty dictionary called models\n",
        "  models = dict()\n",
        "\n",
        "  # Add key:value pairs to dictionary with key as ModelName and value as instantiations (no arguments) for LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, SVC, and GaussianNB base models\n",
        "  # Hint: models['ModelName'] = Classifier()\n",
        "  models['LR'] = LogisticRegression() \n",
        "  models['KNN'] = KNeighborsClassifier() \n",
        "  models['DT'] = DecisionTreeClassifier()\n",
        "  models['SVM'] = SVC()\n",
        "  models['Bayes'] = GaussianNB()\n",
        "\n",
        "  # Add key:value pair to dictionary with key called Stacking and value that calls get_stacking() custom function\n",
        "  models['Stacking'] = get_stacking()\n",
        "\n",
        "  # return dictionary\n",
        "  return models"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flSG4dH1zCTK",
        "colab_type": "text"
      },
      "source": [
        "## Custom function # 3: evaluate_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGLKRr0j5Nit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define evaluate_model:\n",
        "def evaluate_model(model):\n",
        "\n",
        "  # Create RepeatedStratifiedKFold cross-validator with 10 folds, 3 repeats and a seed of 1.\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "  # Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'accuracy' scoring, cross validator 'cv', n_jobs=-1, and error_score set to 'raise'\n",
        "  scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "  # return scores\n",
        "  return scores"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5wmC-TH7B7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign get_models() to a variable called models\n",
        "models = get_models()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02tyK34l2eh7",
        "colab_type": "text"
      },
      "source": [
        "## Python Dictionary Review:\n",
        "- The items() method is used to return the list with all dictionary keys with values. Parameters: This method takes no parameters. Returns: A view object that displays a list of a given dictionary's (key, value) tuple pair.\n",
        "- For our purposes, we'll use the dictionary created when we call the get_models() custom function in a for loop to iterate over each key:value pair and store the results.\n",
        "- Then, we will plot the results as a `boxplot` for comparison using `seaborn`.\n",
        "\n",
        "1. `sns.boxplot()` arguments:\n",
        " - `x`: Names of the variables in the data\n",
        " - `y`: Names of the variables in the data\n",
        " - `showmeans`: Whether or not to show mark at the mean of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzXmYt1o6FWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "885ed9f6-e663-4dba-9e4f-50d538c763e3"
      },
      "source": [
        "# Evaluate the models and store results\n",
        "# Create an empty list for the results\n",
        "results = list()\n",
        "\n",
        "# Create an empty list for the model names\n",
        "names = list()\n",
        "\n",
        "# Create a for loop that iterates over each name, model in models dictionary \n",
        "for name, model in models.items():\n",
        "\n",
        "\t# Call evaluate_model(model) and assign it to variable called scores\n",
        "\tscores = evaluate_model(model)\n",
        " \n",
        "  # Append output from scores to the results list\n",
        "\tresults.append(scores)\n",
        " \n",
        "  # Append name to the names list\n",
        "\tnames.append(name)\n",
        " \n",
        "  # Print name, mean and standard deviation of scores:\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        " \n",
        "# Plot model performance for comparison using names for x and results for y and setting showmeans to True\n",
        "sns.boxplot(x=names, y=results, showmeans=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">LR 0.773 (0.040)\n",
            ">KNN 0.717 (0.040)\n",
            ">DT 0.697 (0.062)\n",
            ">SVM 0.757 (0.040)\n",
            ">Bayes 0.759 (0.055)\n",
            ">Stacking 0.770 (0.046)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4d55aae80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwc_6_Qf4amu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Fiddling with custom function - adding hyperparam tuning in grid/random for loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGs0mMGZmmCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imported libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "import sys\n",
        "\n",
        "# create another custom function to get best hyperparams\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "X = train.drop(['DEFCON_Level','ID'],axis=1)\n",
        "y = train['DEFCON_Level']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#For classification \n",
        "\n",
        "#Random Search\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('classifier',XGBClassifier())])\n",
        "params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "random_search = RandomizedSearchCV(xgb_pipeline, param_distributions=params, n_iter=100,\n",
        "                                   scoring='f1_weighted', n_jobs=4, verbose=3, random_state=1001 )\n",
        "random_search.fit(X_train,y_train)\n",
        "#OR\n",
        "\n",
        "#Grid Search\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('classifier',XGBClassifier())])\n",
        "gbm_param_grid = {\n",
        "    'classifier__learning_rate': np.array([0.01,0.001]),\n",
        "    'classifier__n_estimators': np.array([100,200,300,400]),\n",
        "    'classifier__subsample': np.array([0.7,0.8,0.9]),\n",
        "    'classifier__max_depth': np.array([10,11,12,13,14,15,16,17]),\n",
        "    'classifier__lambda': np.array([1]),\n",
        "    'classifier__gamma': np.array([0])\n",
        "    #'classifier__colsample_bytree': np.arange(0,1.1,.2)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=xgb_pipeline, param_grid=gbm_param_grid, n_jobs= -1,\n",
        "                         scoring='f1_weighted', verbose=10)\n",
        "\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n",
        "#Print out best parameters\n",
        "print(random_search.best_params_)\n",
        "print(grid_search.best_params_)\n",
        "#Print out scores on validation set\n",
        "print(random_search.score(X_test,y_test))\n",
        "print(grid_search.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMZ8gTb6LGCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "d8fb3345-d165-418a-fa37-5e2180ae42cd"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# try improving with tuned hyperparams\n",
        "# Define get_stacking():\n",
        "def get_stacking():\n",
        "\n",
        "\t# Create an empty list for the base models called layer1\n",
        "  layer1 = list()\n",
        "\n",
        "  # Append tuple with classifier name and instantiations (no arguments) for LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, SVC, and GaussianNB base models\n",
        "  # Hint: layer1.append(('ModelName', Classifier()))\n",
        "  layer1.append(('LR', LogisticRegression(C=3792, random_state=42)))\n",
        "  # layer1.append(('KNN', KNeighborsClassifier(n_neighbors=19)))\n",
        "  # layer1.append(('DT', DecisionTreeClassifier(max_depth=80, max_features=6)))\n",
        "  # layer1.append(('SVM', SVC()))\n",
        "  # layer1.append(('Bayes', GaussianNB()))\n",
        "\n",
        "  # Instantiate Logistic Regression as meta learner model called layer2\n",
        "  layer2 = LogisticRegression()\n",
        "\n",
        "\t# Define StackingClassifier() called model passing layer1 model list and meta learner with 5 cross-validations\n",
        "  model = StackingClassifier(estimators=layer1, final_estimator=layer2, cv=5)\n",
        "\n",
        "  # return model\n",
        "  return model\n",
        "\n",
        "# Define get_models():\n",
        "def get_models():\n",
        "\n",
        "  # Create empty dictionary called models\n",
        "  models = dict()\n",
        "\n",
        "  # Add key:value pairs to dictionary with key as ModelName and value as instantiations (no arguments) for LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, SVC, and GaussianNB base models\n",
        "  # Hint: models['ModelName'] = Classifier()\n",
        "  models['LR'] = LogisticRegression()# C=3792, random_state=42) \n",
        "  # models['KNN'] = KNeighborsClassifier() # n_neighbors=19) \n",
        "  # models['DT'] = DecisionTreeClassifier() # max_depth=80, max_features=6)\n",
        "  # models['SVM'] = SVC()\n",
        "  # models['Bayes'] = GaussianNB()\n",
        "\n",
        "  # Add key:value pair to dictionary with key called Stacking and value that calls get_stacking() custom function\n",
        "  models['Stacking'] = get_stacking()\n",
        "\n",
        "  # return dictionary\n",
        "  return models\n",
        "\n",
        "# Custom function #4 -->\n",
        "# Define best_model:\n",
        "def best_model(name, model):\n",
        "  pipe = Pipeline([('scaler', StandardScaler()), ('classifier',model)])\n",
        "  # print(model)\n",
        "  if name == 'LR':\n",
        "    param_grid = {'classifier__penalty' : ['l1', 'l2'],\n",
        "                'classifier__C' : np.logspace(-4, 4, 20),\n",
        "                'classifier__solver' : ['liblinear']} \n",
        "    # Create grid search object\n",
        "    # this uses k-fold cv\n",
        "    clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "\n",
        "    # Fit on data\n",
        "\n",
        "    best_clf = clf.fit(X, y)\n",
        "  # Create RepeatedStratifiedKFold cross-validator with 10 folds, 3 repeats and a seed of 1.\n",
        "  # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "  # Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'accuracy' scoring, cross validator 'cv', n_jobs=-1, and error_score set to 'raise'\n",
        "  # scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "  \n",
        "  # return scores\n",
        "  # return scores\n",
        "    best_hyperparams = best_clf.best_estimator_.get_params()['classifier']\n",
        "    print(best_hyperparams)\n",
        "    return best_hyperparams\n",
        "\n",
        "# Evaluate the models and store results\n",
        "# Create an empty list for the best results\n",
        "results = list()\n",
        "best = list()\n",
        "\n",
        "# Create an empty list for the model names\n",
        "names = list()\n",
        "\n",
        "# Create a for loop that iterates over each name, model in models dictionary \n",
        "for name, model in models.items():\n",
        "  \n",
        "  # Call best(model) and assign it to variable called hyperparams\n",
        "  hyperparams = best_model(name, model)\n",
        "\n",
        "  # Call evaluate_model(model) and assign it to variable called scores\n",
        "  scores = evaluate_model(model)\n",
        "  # Append output from scores to the results list\n",
        "  results.append(scores)\n",
        "  \n",
        "  # Append hyperparams to the best list\n",
        "  best.append(hyperparams)\n",
        "\n",
        "  # Append name to the names list\n",
        "  names.append(name)\n",
        "\n",
        "  # Print best hyperparameters\n",
        "  print(best)\n",
        "  \n",
        "  # Print name, mean and standard deviation of scores:\n",
        "  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "\n",
        "\n",
        "# Plot model performance for comparison using names for x and results for y and setting showmeans to True\n",
        "# sns.boxplot(x=names, y=results, showmeans=True)\n",
        "#LR 0.773 (0.040)\n",
        "#KNN 0.717 (0.040)\n",
        "#DT 0.697 (0.058)\n",
        "#SVM 0.757 (0.040)\n",
        "#Bayes 0.759 (0.055)\n",
        "#Stacking 0.765 (0.045)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "[LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)]\n",
            ">LR 0.773 (0.040)\n",
            "[LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), None]\n",
            ">KNN 0.717 (0.040)\n",
            "[LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), None, None]\n",
            ">DT 0.700 (0.051)\n",
            "[LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), None, None, None]\n",
            ">SVM 0.757 (0.040)\n",
            "[LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), None, None, None, None]\n",
            ">Bayes 0.759 (0.055)\n",
            "[LogisticRegression(C=0.23357214690901212, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), None, None, None, None, None]\n",
            ">Stacking 0.765 (0.048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWLIMvaWyThD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "7ac124d5-73ff-4ed2-f751-1a363104363b"
      },
      "source": [
        "# Let's try tuning some hyperparameters from each of the models to find the best ones, maybe our results will improve!!!\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#hyper parameter tuning.Selecting best K\n",
        "neighbors = [x for x in range(1,50) if x % 2 != 0]\n",
        "# empty list that will hold cv scores\n",
        "cv_scores = []\n",
        "for k in neighbors:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
        "    cv_scores.append(scores.mean())\n",
        "#graphical view\n",
        "#misclassification error\n",
        "MSE = [1-x for x in cv_scores]\n",
        "#optimal K\n",
        "optimal_k_index = MSE.index(min(MSE))\n",
        "optimal_k = neighbors[optimal_k_index]\n",
        "print(optimal_k)\n",
        "# plot misclassification error vs k\n",
        "plt.plot(neighbors, MSE)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Misclassification Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiz73Z630o4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd03952f-8ad1-410e-9c79-4b8b0b7d4d50"
      },
      "source": [
        "# Create first pipeline for base without reducing features.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# pipe = Pipeline([('classifier', DecisionTreeClassifier())])\n",
        "# pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
        "\n",
        "# Create list of classifiers to tune\n",
        "clfs = [LogisticRegression,KNeighborsClassifier,DecisionTreeClassifier,SVC,GaussianNB]\n",
        "\n",
        "# Create empty list for best params\n",
        "best = list()\n",
        "for clf in clfs:\n",
        "  pipe = Pipeline([('scaler', StandardScaler()), ('classifier',clf())])\n",
        "  # print(pipe)\n",
        "  # Create param grids\n",
        "  if clf == 'LogisticRegression':\n",
        "    param_grid = {'classifier__penalty' : ['l1', 'l2'],\n",
        "                'classifier__C' : np.logspace(-4, 4, 20),\n",
        "                'classifier__solver' : ['liblinear']} \n",
        "    tune = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "    best_clf = tune.fit(X, y)\n",
        "    best.append(best_clf.best_estimator_.get_params())# ['classifier'])\n",
        "\n",
        "print(best)# '>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "   \n",
        "\n",
        "\n",
        "#param_grid = [\n",
        "    # {'classifier' : [LogisticRegression()],\n",
        "    # 'classifier__penalty' : ['l1', 'l2'],\n",
        "    #'classifier__C' : np.logspace(-4, 4, 20),\n",
        "    #'classifier__solver' : ['liblinear']} ,\n",
        " #   {'classifier' : [DecisionTreeClassifier()],\n",
        " #   'classifier__max_depth' : list(range(10,101,10)),\n",
        " #   'classifier__max_features' : list(range(6,32,5))}\n",
        "#]\n",
        "\n",
        "# Create grid search object\n",
        "# this uses k-fold cv\n",
        "#clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "\n",
        "# Fit on data\n",
        "\n",
        "#best_clf = clf.fit(X, y)\n",
        "\n",
        "# make a list/library and append to lists the best_clf and name as loop through the parameters, then can print it out to re-run the stacking classifier to see if improves?"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smAACLXbGQ6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e38a76a4-14e1-4c19-83f6-305aa7121238"
      },
      "source": [
        "best_clf.best_estimator_.get_params()['classifier']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=80, max_features=6, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5OpzmNX4mn2",
        "colab_type": "text"
      },
      "source": [
        "## Fiddling above\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUqeWsol5RAt",
        "colab_type": "text"
      },
      "source": [
        "## **Observation**\n",
        "- Recall that we want to do better than 77% with a Stacking Classifier to consider it an improvement over this baseline logistic regression model and, although close, we did not achieve that with this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj8WeJR__bUo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Stacking Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftxDhyDq2lrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.svm import SVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqDHD8A_nhPB",
        "colab_type": "text"
      },
      "source": [
        "## **2nd Dataset**\n",
        "\n",
        "\n",
        "The second dataset we'll use is a CSV file named `abalone.csv`, which contains data on physical measurements of abalone shells used to determine the age of the abalone.  It contains the following columns:\n",
        "\n",
        "- `Sex`: M, F, and I (infant) - (removed for our purposes)\n",
        "- `Length`: Longest shell measurement (mm)\n",
        "- `Diameter`: Perpendicular to length (mm)\n",
        "- `Height`: with meat in shell (mm)\n",
        "- `Whole weight`: whole abalone (grams)\n",
        "- `Shucked weight`: weight of meat (grams)\n",
        "- `Viscera weight`: gut weight (grams)\n",
        "- `Shell weight`: after being dried (grams)\n",
        "- `Rings`: +1.5 gives the age in years\n",
        "\n",
        "\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwNnn3ZKrh1o",
        "colab_type": "text"
      },
      "source": [
        "### **Get the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4LeaM4PzyAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the dataset as Pandas DataFrame\n",
        "abalone = pd.read_csv('https://github.com/datacamp/Applied-Machine-Learning-Ensemble-Modeling-live-training/blob/master/data/abalone.csv?raw=true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfsmhIBdApVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "354f461f-c6a6-473f-8612-d0b493ec6bff"
      },
      "source": [
        "# Look at data using the info() function\n",
        "abalone.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4177 entries, 0 to 4176\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Sex             4177 non-null   object \n",
            " 1   Length          4177 non-null   float64\n",
            " 2   Diameter        4177 non-null   float64\n",
            " 3   Height          4177 non-null   float64\n",
            " 4   Whole weight    4177 non-null   float64\n",
            " 5   Shucked weight  4177 non-null   float64\n",
            " 6   Viscera weight  4177 non-null   float64\n",
            " 7    Shell weight   4177 non-null   float64\n",
            " 8   Rings           4177 non-null   int64  \n",
            "dtypes: float64(7), int64(1), object(1)\n",
            "memory usage: 293.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZAeIFGwBhe6",
        "colab_type": "text"
      },
      "source": [
        "## **Observations:** \n",
        "- Here, there are no missing values.  Again, that is not typical.\n",
        "- There is a mixture of object, float, and integers with the first column being `object` (categorical), the next 7 `float64` and the last 'int64`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D4Gfh08Avb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "139da6dc-489f-45f6-fe51-ce60f5fd3d85"
      },
      "source": [
        "# Look at data using the describe() function\n",
        "abalone.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "      <td>4177.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.523992</td>\n",
              "      <td>0.407881</td>\n",
              "      <td>0.139516</td>\n",
              "      <td>0.828742</td>\n",
              "      <td>0.359367</td>\n",
              "      <td>0.180594</td>\n",
              "      <td>0.238831</td>\n",
              "      <td>9.933684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.120093</td>\n",
              "      <td>0.099240</td>\n",
              "      <td>0.041827</td>\n",
              "      <td>0.490389</td>\n",
              "      <td>0.221963</td>\n",
              "      <td>0.109614</td>\n",
              "      <td>0.139203</td>\n",
              "      <td>3.224169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.115000</td>\n",
              "      <td>0.441500</td>\n",
              "      <td>0.186000</td>\n",
              "      <td>0.093500</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.545000</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.799500</td>\n",
              "      <td>0.336000</td>\n",
              "      <td>0.171000</td>\n",
              "      <td>0.234000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.615000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.165000</td>\n",
              "      <td>1.153000</td>\n",
              "      <td>0.502000</td>\n",
              "      <td>0.253000</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.815000</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>1.130000</td>\n",
              "      <td>2.825500</td>\n",
              "      <td>1.488000</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>1.005000</td>\n",
              "      <td>29.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Length     Diameter  ...   Shell weight        Rings\n",
              "count  4177.000000  4177.000000  ...    4177.000000  4177.000000\n",
              "mean      0.523992     0.407881  ...       0.238831     9.933684\n",
              "std       0.120093     0.099240  ...       0.139203     3.224169\n",
              "min       0.075000     0.055000  ...       0.001500     1.000000\n",
              "25%       0.450000     0.350000  ...       0.130000     8.000000\n",
              "50%       0.545000     0.425000  ...       0.234000     9.000000\n",
              "75%       0.615000     0.480000  ...       0.329000    11.000000\n",
              "max       0.815000     0.650000  ...       1.005000    29.000000\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDGc7PPBBkGX",
        "colab_type": "text"
      },
      "source": [
        "## **Observations:** \n",
        "- Notice that the min of the `Height` column is zero.  Even though there are no missing values, this is indicative of the measurements for that feature having not been captured.\n",
        "- Again, the printout makes it appear as if all numeric values are float.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVGtuWoDAvl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d34d17e7-0a8b-4dc2-d7aa-e646c695d4f8"
      },
      "source": [
        "# Print the first 5 rows of the data using the head() function\n",
        "abalone.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Sex  Length  Diameter  ...  Viscera weight   Shell weight  Rings\n",
              "0   M   0.455     0.365  ...          0.1010          0.150     15\n",
              "1   M   0.350     0.265  ...          0.0485          0.070      7\n",
              "2   F   0.530     0.420  ...          0.1415          0.210      9\n",
              "3   M   0.440     0.365  ...          0.1140          0.155     10\n",
              "4   I   0.330     0.255  ...          0.0395          0.055      7\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnmVoSl8BmMY",
        "colab_type": "text"
      },
      "source": [
        "## **Observation:**\n",
        "- Printing out the first 5 rows, we see that the 1st column is the only non-numeric feature in this dataset and is aligned with the `object` datatype as we saw above when we called `.info()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPfVhWzRrm_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0296ffb-4871-4f7c-c150-b6ba6bd9e17a"
      },
      "source": [
        "# Convert Pandas DataFrame to numpy array - Return only the values of the DataFrame with DataFrame.to_numpy()\n",
        "abalone = abalone.to_numpy()\n",
        "\n",
        "# Create X matrix and y (target) array using slicing [row_start:row_end, 1:target_col],[row_start:row_end, target_col] - Removing 1st column by starting at index 1\n",
        "X, y = abalone[:, 1:-1], abalone[:, -1]\n",
        "\n",
        "# Print X matrix and y (target) array dimensions using .shape\n",
        "print('Shape: %s, %s' % (X.shape,y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (4177, 7), (4177,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ6CHfsVrpE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert y (target) array to 'float32' using .astype()\n",
        "y = y.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bYvtBfSF7k7",
        "colab_type": "text"
      },
      "source": [
        "## **Creating a Naive Regressor**\n",
        "Here we'll use the `DummyRegressor` from `sklearn`.  This creates a so-called 'naive' regressor and is simply a model that predicts a single value for all of the rows, regardless of their original value.  \n",
        "\n",
        "1. `DummyRegressor()` arguments:\n",
        " - `strategy`: Strategy to use to generate predictions.\n",
        "\n",
        "2. `RepeatedKFold()` arguments:\n",
        " - `n_splits`: Number of folds.\n",
        " - `n_repeats`: Number of times cross-validator needs to be repeated.\n",
        " - `random_state`: Controls the generation of the random states for each repetition. Pass an int for reproducible output across multiple function calls.  (This is an equivalent argument to np.random.seed above, but will be specific to this naive model.)\n",
        "\n",
        "3. `cross_val_score()` arguments:\n",
        " - The model to use.\n",
        " - The data to fit. (X)\n",
        " - The target variable to try to predict. (y)\n",
        " - `scoring`: A single string scorer callable object/function such as 'accuracy' or 'roc_auc'.  See https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter for more options.\n",
        " - `cv`: Cross-validation splitting strategy (default is 5)\n",
        " - `n_jobs`: Number of CPU cores used when parallelizing.  Set to -1 helps to avoid non-convergence errors.\n",
        " - `error_score`: Value to assign to the score if an error occurs in estimator fitting. If set to ‘raise’, the error is raised. If a numeric value is given, FitFailedWarning is raised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAJdcu_Hrrg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf665256-2c42-4a93-8166-a4e597d6bd7d"
      },
      "source": [
        "# Evaluate naive\n",
        "\n",
        "# Instantiate a DummyRegressor with 'median' strategy\n",
        "naive = DummyRegressor(strategy='median')\n",
        "\n",
        "# Create RepeatedKFold cross-validator with 10 folds, 3 repeats and a seed of 1.\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'neg_mean_absolute_error' scoring, cross validator, n_jobs=-1, and error_score set to 'raise'\n",
        "n_scores = cross_val_score(naive, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# Print mean and standard deviation of n_scores:\n",
        "print('Baseline: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: -2.372 (0.119)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYQmsCQHcdJ",
        "colab_type": "text"
      },
      "source": [
        "## **Observation** \n",
        "- We want to do better than -2.37 to consider any other models as an improvement to a totally naive regressor model with the Abalone dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfiEdoUMHo-q",
        "colab_type": "text"
      },
      "source": [
        "## **Creating a Baseline Regressor**\n",
        "Now we'll create a baseline regressor, one that seeks to correctly predict the value for each observation.  Since the target variable is continuous, we'll instantiate a Support Vector Regression model.\n",
        "\n",
        "1. `SVR()` arguments:\n",
        " - `kernel`: Specifies the kernel type to be used in the algorithm.\n",
        " - `gamma`:  Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. \n",
        " - `C`: Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFip40FPrvOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73ac5bbc-de1a-47af-bb24-5c87243ea2da"
      },
      "source": [
        "# Evaluate baseline model\n",
        "\n",
        "# Instantiate a Support Vector Regressor with 'rbf' kernel, gamma set to 'scale', and regularization parameter set to 10\n",
        "model = SVR(kernel='rbf',gamma='scale',C=10)\n",
        "\n",
        "# Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'neg_mean_absolute_error' scoring, cross validator 'cv', n_jobs=-1, and error_score set to 'raise'\n",
        "m_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# Print mean and standard deviation of m_scores: \n",
        "print('Good: %.3f (%.3f)' % (mean(m_scores), std(m_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Good: -1.483 (0.075)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PMtVARKzBX",
        "colab_type": "text"
      },
      "source": [
        "## **Observation**\n",
        "- We want to do better than -1.48 with a Stacking Regressor to consider it an improvement over this baseline support vector regression model with the Abalone dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-OGF_7bupzn",
        "colab_type": "text"
      },
      "source": [
        "## **Getting started with Stacking Regressor**\n",
        "- We're going to compare several additional baseline regressors to see if they perform better than SVR we just trained previously.\n",
        "- We'll start by importing additional packages that we'll need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxbxTPkPrkNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare machine learning models for regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import StackingRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yixxr2JLN9UP",
        "colab_type": "text"
      },
      "source": [
        "## Create custom functions\n",
        "1. get_stacking() - This function will create the layers of our `StackingRegressor()`.\n",
        "2. get_models() - This function will create a dictionary of models to be evaluated.\n",
        "3. evaluate_model() - This function will evaluate each of the models to be compared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdF239ZRN92B",
        "colab_type": "text"
      },
      "source": [
        "## Custom function # 1: get_stacking()\n",
        "1. `StackingRegressor()` arguments:\n",
        " - `estimators`: List of baseline regressors\n",
        " - `final_estimator`: Defined meta regressor \n",
        " - `cv`: Number of cross validations to perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoRNxZSj72bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_stacking():\n",
        "def get_stacking():\n",
        "\n",
        "\t# Create an empty list for the base models called layer1\n",
        "  layer1 = list()\n",
        "\n",
        "  # Append tuple with classifier name and instantiations (no arguments) for KNeighborsRegressor, DecisionTreeRegressor, and SVR base models\n",
        "  # Hint: layer1.append(('ModelName', Classifier()))\n",
        "  layer1.append(('KNN', KNeighborsRegressor()))\n",
        "  layer1.append(('DT', DecisionTreeRegressor()))\n",
        "  layer1.append(('SVM', SVR()))\n",
        "\n",
        "  # Instantiate Linear Regression as meta learner model called layer2\n",
        "  layer2 = LinearRegression()\n",
        "\n",
        "\t# Define StackingClassifier() called model passing layer1 model list and meta learner with 5 cross-validations\n",
        "  model = StackingRegressor(estimators=layer1, final_estimator=layer2, cv=5)\n",
        "\n",
        "  # return model\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KClsJExROLAZ",
        "colab_type": "text"
      },
      "source": [
        "## Custom function # 2: get_models()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtYbhE_ps4yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_models():\n",
        "def get_models():\n",
        "\n",
        "  # Create empty dictionary called models\n",
        "  models = dict()\n",
        "\n",
        "  # Add key:value pairs to dictionary with key as ModelName and value as instantiations (no arguments) for KNeighborsRegressor, DecisionTreeRegressor, and SVR base models\n",
        "  # Hint: models['ModelName'] = Classifier()\n",
        "  models['KNN'] = KNeighborsRegressor()\n",
        "  models['DT'] = DecisionTreeRegressor()\n",
        "  models['SVM'] = SVR()\n",
        "\n",
        "  # Add key:value pair to dictionary with key called Stacking and value that calls get_stacking() custom function\n",
        "  models['Stacking'] = get_stacking()\n",
        "\n",
        "  # return dictionary\n",
        "  return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYH3KcjcOc56",
        "colab_type": "text"
      },
      "source": [
        "## Custom function # 3: evaluate_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H95M82gks6EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define evaluate_model:\n",
        "def evaluate_model(model):\n",
        "\n",
        "  # Create RepeatedKFold cross-validator with 10 folds, 3 repeats and a seed of 1.\n",
        "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        " \n",
        "  # Calculate accuracy using `cross_val_score()` with model instantiated, data to fit, target variable, 'neg_mean_absolute_error' scoring, cross validator 'cv', n_jobs=-1, and error_score set to 'raise'\n",
        "\tscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        " \n",
        "  # return scores\n",
        "\treturn scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C6Hw-wj56eK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign get_models() to a variable called models\n",
        "models = get_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZl3DjmU58Lm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "32144743-3ee2-4b32-845d-f25ece18248c"
      },
      "source": [
        "# Evaluate the models and store results\n",
        "# Create an empty list for the results\n",
        "results = list()\n",
        "\n",
        "# Create an empty list for the model names\n",
        "names = list()\n",
        "\n",
        "# Create a for loop that iterates over each name, model in models dictionary \n",
        "for name, model in models.items():\n",
        "\n",
        "\t# Call evaluate_model(model) and assign it to variable called scores\n",
        "\tscores = evaluate_model(model)\n",
        " \n",
        "  # Append output from scores to the results list\n",
        "\tresults.append(scores)\n",
        " \n",
        "  # Append name to the names list\n",
        "\tnames.append(name)\n",
        " \n",
        "  # Print name, mean and standard deviation of scores:\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        " \n",
        "# Plot model performance for comparison using names for x and results for y and setting showmeans to True\n",
        "sns.boxplot(x=names, y=results, showmeans=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">KNN -1.597 (0.064)\n",
            ">DT -2.109 (0.117)\n",
            ">SVM -1.518 (0.077)\n",
            ">Stacking -1.536 (0.069)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f10b46df978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6EKNBV1UOuG",
        "colab_type": "text"
      },
      "source": [
        "## **Observation**\n",
        "- Recall that we want to do better than -1.48  with a Stacking Regressor to consider it an improvement over this baseline SVR and, although close, we did not achieve that with this dataset.\n",
        "- So what else can try to improve our results with stacking?\n",
        "\n",
        "### We'll add another layer to the mix..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9DZ7iyZFxXo",
        "colab_type": "text"
      },
      "source": [
        "## **Double Stacking - 2 Layers**\n",
        "- Can get a little tricky\n",
        "- Just make sure that you name your layers VERY CLEARLY!\n",
        "- Both the last layer (here it's layer 3) and the stacking model will use a call to `StackingRegressor()`\n",
        "- The last layer will combine the 2nd layer with the final estimator while the model will combine the 1st layer with this last layer.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Applied-Machine-Learning-Ensemble-Modeling-live-training/blob/master/assets/DoubleStacking.png?raw=True\" alt = \"Double Stacking\" width=\"90%\">\n",
        "</p>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXvUmmQQF6vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define get_stacking() - adding another layer:\n",
        "def get_stacking():\n",
        "\n",
        "\t# Create an empty list for the 1st layer of base models called layer1\n",
        "  layer1 = list()\n",
        "\n",
        "  # Create an empty list for the 2nd layer of base models called layer2\n",
        "  layer2 = list()\n",
        "\n",
        "  # Append tuple with classifier name and instantiations (no arguments) for KNeighborsRegressor, DecisionTreeRegressor, and SVR base models\n",
        "  # Hint: layer1.append(('ModelName', Classifier()))\n",
        "  layer1.append(('KNN', KNeighborsRegressor()))\n",
        "  layer1.append(('DT', DecisionTreeRegressor()))\n",
        "  layer1.append(('SVM', SVR()))\n",
        "\n",
        "  # Append tuple with classifier name and instantiations (no arguments) for KNeighborsRegressor, DecisionTreeRegressor, and SVR base models\n",
        "  # Hint: layer2.append(('ModelName', Classifier()))\n",
        "  layer2.append(('KNN', KNeighborsRegressor()))\n",
        "  layer2.append(('DT', DecisionTreeRegressor()))\n",
        "  layer2.append(('SVM', SVR()))\n",
        "\n",
        "\t# Define meta learner StackingRegressor() called layer3 passing layer2 model list to estimators, LinearRegression() to final_estimator with 5 cross-validations\n",
        "  layer3 = StackingRegressor(estimators=layer2, final_estimator=LinearRegression(), cv=5)\n",
        "\n",
        "\t# Define StackingClassifier() called model passing layer1 model list to estimators and meta learner (layer3) to final_estimator with 5 cross-validations\n",
        "  model = StackingRegressor(estimators=layer1, final_estimator=layer3, cv=5)\n",
        "\n",
        "  # return model\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnMMqOJ16Bft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign get_models() to a variable called models\n",
        "models = get_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvzSjLOEIKUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "3eb183e7-8443-42d3-8640-19ef9ed37956"
      },
      "source": [
        "# Evaluate the models and store results\n",
        "# Create an empty list for the results\n",
        "results = list()\n",
        "\n",
        "# Create an empty list for the model names\n",
        "names = list()\n",
        "\n",
        "# Create a for loop that iterates over each name, model in models dictionary \n",
        "for name, model in models.items():\n",
        "\n",
        "\t# Call evaluate_model(model) and assign it to variable called scores\n",
        "\tscores = evaluate_model(model)\n",
        " \n",
        "  # Append output from scores to the results list\n",
        "\tresults.append(scores)\n",
        " \n",
        "  # Append name to the names list\n",
        "\tnames.append(name)\n",
        " \n",
        "  # Print name, mean and standard deviation of scores:\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        " \n",
        "# Plot model performance for comparison using names for x and results for y and setting showmeans to True\n",
        "sns.boxplot(x=names, y=results, showmeans=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">KNN -1.597 (0.064)\n",
            ">DT -2.103 (0.112)\n",
            ">SVM -1.518 (0.077)\n",
            ">Stacking -1.545 (0.072)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f10b4c00940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMgN44SwcJPG",
        "colab_type": "text"
      },
      "source": [
        "## **Final Observation**\n",
        "- Adding a layer did not improve results.\n",
        "- Complexity does not always make a better model\n",
        "- Could try different base models to stack for both of the datasets and that may show improvements over baseline.\n",
        "- Generate polynomial features \n",
        "- Try sklearn feature selection\n",
        "- Tune additional hyperparameters for grid search\n",
        "- When there is a tie between a baseline model and a stacked model, choose the simpler model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE4Yv6iXnrS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f70dc3df-f4f3-42dd-8e7b-1d43c8b41a1c"
      },
      "source": [
        "# Delete this cell?\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Fit the model on all available data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make a prediction for first 5 example\n",
        "data = X[1:6]# .reshape(1, -1)\n",
        "yhat = model.predict(data)\n",
        "print('Predicted Class: %s' % str(yhat))\n",
        "print('Actual Class: %s' % str(y[1:6]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Class: [0 1 0 1 0]\n",
            "Actual Class: [0 1 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}